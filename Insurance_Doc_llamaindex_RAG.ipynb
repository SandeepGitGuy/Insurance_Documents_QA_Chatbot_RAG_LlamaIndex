{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama_index transformers\n",
    "#%pip install python-dotenv\n",
    "#%pip install nest_asyncio\n",
    "# %pip install diskcache\n",
    "#%pip install llama-index-embeddings-huggingface\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\keys.env\")\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import  VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "import chromadb\n",
    "\n",
    "import time\n",
    "\n",
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chroma_client = chromadb.PersistentClient(path=r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\chroma.db\")\n",
    "cache = Cache(\"./cache\")\n",
    "nest_asyncio.apply()\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pdf_dir_path = r\"C:\\Users\\sandy\\Downloads\\Insurance_Doc_RAG_With_LangchainLlamaIndex\\New folder\"\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.environ)\n",
    "#print(api_key)\n",
    "\n",
    "# docs = []\n",
    "# for doc in documnet:\n",
    "#     docs.append(doc.text)\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(pdf_dir_path, storage_context):\n",
    "    docs = SimpleDirectoryReader(pdf_dir_path).load_data()\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(),\n",
    "            TitleExtractor(),\n",
    "            HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "            #OpenAIEmbedding(model_name=\"text-embedding-ada-002\"),\n",
    "        ]\n",
    "    )\n",
    "    nodes = pipeline.run(documents=docs)\n",
    "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index():\n",
    "    try:\n",
    "        chroma_collection = chroma_client.create_collection(name=\"Insurance_Doc_RAG_LlamaIndex\")\n",
    "\n",
    "    except Exception as e:\n",
    "        chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex\")\n",
    "\n",
    "    vector_store = ChromaVectorStore(\n",
    "        chroma_collection=chroma_collection,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    index = build_index(pdf_dir_path, storage_context)\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index():\n",
    "    try:\n",
    "         chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"Save the index first\")\n",
    "       \n",
    "\n",
    "    vector_store = ChromaVectorStore(\n",
    "        chroma_collection=chroma_collection,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    index =VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector_Index = VectorStoreIndex()\n",
    "# index = Vector_Index.build_index_from_nodes(nodes=nodes)\n",
    "# Load documents\n",
    "# pdf_dir_path = r\"C:\\Users\\sandy\\Downloads\\Policy+Documents\"\n",
    "# docs = SimpleDirectoryReader(pdf_dir_path).load_data()\n",
    "\n",
    "# with open('vector_store_index.pkl', 'rb') as f:\n",
    "#     loaded_index = pickle.load(f)\n",
    "\n",
    "# if loaded_index is not None:\n",
    "#     index = loaded_index\n",
    "# else:\n",
    "#     index = build_index(pdf_dir_path)\n",
    "#     with open(\"vector_store_index.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever()\n",
    "\n",
    "# results = retriever.retrieve(\"What is the procedure to claim the insurance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     print(res.node.metadata[\"document_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# %pip install tf_keras\n",
    "#%pip uninstall keras\n",
    "#%pip install keras==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_comm(query, index):\n",
    "    retriever = index.as_retriever()\n",
    "    results = retriever.retrieve(query)\n",
    "\n",
    "    system_message = f\"\"\"You are an Question answering expert. The user will ask you a question/query. \n",
    "    The Question is : {query}\n",
    "    Now, the Documents related to the question is : {[res.node.text for res in results]}\n",
    "    If the question is related to the document, answer it using the information in the document.\n",
    "    If the question is not related to the document, answer \"Please contact the insurance company/agent as I am not able to answer the question\".\n",
    "    If you answer the question, please provide the relevant document as reference.\n",
    "    Reference Format : Page Number | Document Name.\n",
    "    Page Numbers is {[res.node.metadata['page_label'] for res in results]}\n",
    "    Document Names is {[res.node.metadata['document_title'] for res in results]}\n",
    "    Example:\n",
    "    Reference 1 : Page 4 | Accidental Death Benefit Claims Procedure and Exclusions\n",
    "    etc.\n",
    "    Use all the info retrieved and if used then give the reference. Multiple references can be used.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\", system_prompt=system_message)\n",
    "\n",
    "    query_engine = index.as_query_engine(response_mode=\"compact\", similirty_top_k=3, llm=llm, node_postprocessors=[rerank])\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Insurance Documentation Chatbot. Please enter your query.\n",
      "User:  what is Accelerated Critical Illness Benefit?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Searching in Cache... Please wait!\n",
      "Data not found in Cache. Searching in Documents...\n",
      "Data Found. Retrieving relevant information...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Accelerated Critical Illness Benefit is a feature of an insurance policy that provides a benefit equal to the Sum Assured in the event that the Scheme Member is diagnosed with any of the covered Critical Illnesses during the Policy Term. Upon diagnosis, the policy will terminate, and all benefits will expire. The covered Critical Illnesses include conditions such as Myocardial Infarction, Cancer of Specified Severity, Stroke, and several others as listed in the policy document. \n",
      "\n",
      "Reference: Page 7 | Comprehensive Guide to Insurance Benefits: Death, Accidental Death, and Critical Illness Coverage.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thanks for using Insurance Documentation Chatbot. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    index = load_index()\n",
    "    #print(\"load path\")\n",
    "except:\n",
    "    index = save_index()\n",
    "    #print(\"save path\")\n",
    "print(\"Welcome to Insurance Documentation Chatbot. Please enter your query.\")\n",
    "query = input()\n",
    "print(\"User: \", query)\n",
    "print(\"-\"*100)\n",
    "time.sleep(1)\n",
    "print(\"Searching in Cache... Please wait!\")\n",
    "if cache.get(query) is not None:\n",
    "    print(\"Data found in Cache. Retrieving relevant information...\")\n",
    "    time.sleep(1)\n",
    "    response = cache.get(query)\n",
    "else:\n",
    "    print(\"Data not found in Cache. Searching in Documents...\")\n",
    "    time.sleep(1)\n",
    "    print(\"Data Found. Retrieving relevant information...\")\n",
    "    response = query_comm(query, index)\n",
    "    cache.set(query, response, expire=600)\n",
    "print(\"-\"*100)\n",
    "print(response)\n",
    "print(\"-\"*100)\n",
    "print(\"Thanks for using Insurance Documentation Chatbot. Have a great day!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
